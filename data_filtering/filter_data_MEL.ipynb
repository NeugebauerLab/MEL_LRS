{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook filters mapped PacBio LRS data to remove:\n",
    "    polyadenylated transcripts,\n",
    "    7SK transcripts,\n",
    "    non-unique reads,\n",
    "    and splicing intermeditates\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pysam\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from plotnine import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42 # export pdfs with editable font types in Illustrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data filenames and annotations used for filtering\n",
    "\n",
    "samFiles = ['../0_mapped_data/1_untreated_RSII.sam',\n",
    "            '../0_mapped_data/1_untreated_SQ.sam',\n",
    "            '../0_mapped_data/2_untreated_RSII.sam',\n",
    "            '../0_mapped_data/2_untreated_SQ.sam',\n",
    "            '../0_mapped_data/3_DMSO_RSII.sam',\n",
    "            '../0_mapped_data/3_DMSO_SQ.sam',\n",
    "            '../0_mapped_data/4_DMSO_RSII.sam',\n",
    "            '../0_mapped_data/4_DMSO_SQ.sam']\n",
    "\n",
    "Rn7sk = '../annotation/files/7sk.bed'\n",
    "\n",
    "introns = pd.read_csv('../annotation_files/mm10_VM20_introns.bed', delimiter = '\\t', names =  ['chr', 'start', 'end', 'name', 'score', 'strand']) # annotation of all mm10 introns in BED6 format, downloaded from USCS table browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bed file from introns bed file that contains just the nucleotide -1 to the 5SS\n",
    "introns.loc[introns['strand'] == '+', 'fiveSS'] = introns['start']\n",
    "introns.loc[introns['strand'] == '-', 'fiveSS'] = introns['end']\n",
    "introns.loc[introns['strand'] == '+', 'newStart'] = introns['fiveSS'] - 1\n",
    "introns.loc[introns['strand'] == '-', 'newStart'] = introns['fiveSS']\n",
    "introns.loc[introns['strand'] == '+', 'newEnd'] = introns['fiveSS']\n",
    "introns.loc[introns['strand'] == '-', 'newEnd'] = introns['fiveSS'] + 1\n",
    "\n",
    "# convert coordinates back to integer values\n",
    "introns['newStart'] = introns['newStart'].astype(np.int64)\n",
    "introns['newEnd'] = introns['newEnd'].astype(np.int64)\n",
    "\n",
    "# save as BED6 file\n",
    "introns.to_csv('introns_5SS.bed', \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand'], \n",
    "               header = False)\n",
    "\n",
    "# save as bedtool object for intersect\n",
    "introns_5SS_bedtool = pybedtools.BedTool('introns_5SS.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter out polyadenylated reads from PacBio nascent RNA LRS data\n",
    "\n",
    "def filter_polyA(mapped_reads_file):\n",
    "    \n",
    "    def append_id(mapped_reads_file):\n",
    "        name, ext = os.path.splitext(mapped_reads_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='polyAfiltered', ext=ext)\n",
    "    \n",
    "    output = open(append_id(mapped_reads_file), 'w')\n",
    "    #keep = [] # make an empty list of reads to keep\n",
    "    \n",
    "    with open(mapped_reads_file, 'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            col = line.split('\\t')\n",
    "            if col[0][0] == '@': # write header lines into output file\n",
    "                output.write(line + '\\n')\n",
    "                continue\n",
    "            cigar = col[5] # gets cigar string from SAM file\n",
    "\n",
    "            if cigar.count('S') >= 1: # searches for S in cigar string indicating soft-clipped\n",
    "                index = cigar.find('S') # finds position of S in cigar string\n",
    "                if index <= 3: # if index is small then clipped bases are on the front of the read...should be polyT\n",
    "                    length_clipped = int(cigar.split(\"S\")[0]) # get position of first S in cigar string\n",
    "                    clipped_bases = col[9][:length_clipped] # get sequence of clipped bases from start to first S\n",
    "                    if  clipped_bases.count('T') >= 4 and \\\n",
    "                        clipped_bases.count('T')/len(clipped_bases) >=0.9: # if minimum 4 T's and T content is >90% of soft-clipped bases\n",
    "                            continue # skip lines that have polyT at beginning of read\n",
    "                    else:\n",
    "                        output.write(line + '\\n') # print lines that do not have polyT\n",
    "\n",
    "                if (index + 1) == len(cigar): # if the S is at the end of the cigar string\n",
    "                    m = re.search('[0-9]{0,3}(?=S)' , cigar) # find number before last S in cigar string\n",
    "                    length_clipped = int(m.group(0)) #get length of clipped bases at end of read...should be polyA\n",
    "                    clipped_bases = col[9][:(-1 * (length_clipped + 1)):-1] # get sequence of clipped bases before last S\n",
    "                    if  clipped_bases.count('A') >= 4 and \\\n",
    "                        clipped_bases.count('A')/len(clipped_bases) >=0.9: # if minimum 4 A's and A content is >90% of soft-clipped bases\n",
    "\n",
    "                            continue # do not write lines that have polyA at end of read\n",
    "                    else:\n",
    "                        output.write(line + '\\n') # write lines that do not have polyA at the end\n",
    "            else:\n",
    "                output.write(line + '\\n')\n",
    "\n",
    "#     reads_bedtool = BedTool(keep)\n",
    "#     reads_bedtool.saveas('test_filtered.bed')\n",
    "#     return reads_bedtool\n",
    "    f.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for filtering splicing intermediates from each data file\n",
    "def filter_splicing_intermediates(bed_file):\n",
    "    \n",
    "    def name_file_no_splicing_int(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='no_splicing_int', ext=ext)\n",
    "    \n",
    "    def name_file_splicing_int(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='splicing_int', ext=ext)\n",
    "    \n",
    "    # first open and reorder coordinates of bed file to put 3'end in position for intersection\n",
    "    data = pd.read_csv(bed_file, delimiter = '\\t', names =  ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'])\n",
    "    data.loc[data['strand'] == '+', 'threeEnd'] = data['end']\n",
    "    data.loc[data['strand'] == '-', 'threeEnd'] = data['start']\n",
    "    data.loc[data['strand'] == '+', 'fiveEnd'] = data['start']\n",
    "    data.loc[data['strand'] == '-', 'fiveEnd'] = data['end']\n",
    "    data.loc[data['strand'] == '+', 'newStart'] = data['threeEnd'] - 1\n",
    "    data.loc[data['strand'] == '-', 'newStart'] = data['threeEnd']\n",
    "    data.loc[data['strand'] == '+', 'newEnd'] = data['threeEnd']\n",
    "    data.loc[data['strand'] == '-', 'newEnd'] = data['threeEnd'] + 1\n",
    "\n",
    "    # convert coordinates back to integer values\n",
    "    data['newStart'] = data['newStart'].astype(np.int64)\n",
    "    data['newEnd'] = data['newEnd'].astype(np.int64)\n",
    "    data['fiveEnd'] = data['fiveEnd'].astype(np.int64)\n",
    "    data['threeEnd'] = data['threeEnd'].astype(np.int64)\n",
    "    \n",
    "    # save a temporary bed file with data 3'end coordinates\n",
    "    data.to_csv('tmp.bed', \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'], \n",
    "               header = False)\n",
    "        \n",
    "    # intersect data 3'end with intron 5'SS coordinates to get splicing intermediates and non-intermediates\n",
    "    tmp_bedfile = open('tmp.bed')\n",
    "    data_bedtool = pybedtools.BedTool(tmp_bedfile)\n",
    "    intersect1 = data_bedtool.intersect(introns_5SS_bedtool, u = True).saveas('tmp_splicing_int.bed')\n",
    "    \n",
    "    tmp_bedfile = open('tmp.bed')\n",
    "    data_bedtool = pybedtools.BedTool(tmp_bedfile)\n",
    "    intersect2 = data_bedtool.intersect(introns_5SS_bedtool, v = True).saveas('tmp_no_splicing_int.bed')\n",
    "\n",
    "    # reorder coordinates of data files\n",
    "    data1 = pd.read_csv('tmp_splicing_int.bed', delimiter = '\\t', names =  ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'])\n",
    "    data1.to_csv(name_file_splicing_int(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)\n",
    "    \n",
    "    data2 = pd.read_csv('tmp_no_splicing_int.bed', delimiter = '\\t', names =  ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'])\n",
    "    data2.to_csv(name_file_no_splicing_int(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)\n",
    "    \n",
    "    # clean up temp files\n",
    "    os.remove('tmp.bed')\n",
    "    os.remove('tmp_no_splicing_int.bed')\n",
    "    os.remove('tmp_splicing_int.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for filtering non-unique readnames from each data file\n",
    "def filter_nonunique_reads(bed_file):\n",
    "    \n",
    "    def name_unique_reads(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='unique', ext=ext)\n",
    "    \n",
    "    # first open and reorder coordinates of bed file to put 3'end in position for intersection\n",
    "    all_data = pd.read_csv(bed_file, delimiter = '\\t', names =  ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'])\n",
    "    grouped = all_data.groupby(['name']).size().to_frame(name = 'count').reset_index()\n",
    "\n",
    "    # get read names that are unique and filter to keep only reads which have name count == 1\n",
    "    is_unique =  grouped['count'] == 1\n",
    "    unique = grouped[is_unique]\n",
    "    unique_names = pd.Series(unique['name'].values) # create a series of readnames that have occur only once\n",
    "\n",
    "    data_is_unique = all_data['name'].isin(unique_names)\n",
    "    data_unique = all_data[data_is_unique] # filter data for readnames that are unique\n",
    "    \n",
    "    # save unique reads to a new file\n",
    "    data_unique.to_csv(name_unique_reads(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter polyadenylated reads\n",
    "for file in samFiles:\n",
    "    filter_polyA(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SAM files to BAM  files for further filtering\n",
    "SAM = []\n",
    "for file in glob.glob('./*_polyAfiltered.sam'):\n",
    "    SAM.append(file)\n",
    "\n",
    "for samfile in SAM:\n",
    "    name, ext = os.path.splitext(samfile)\n",
    "    bamfile = \"{name}{ext}\".format(name=name, ext='.bam')\n",
    "    pysam.view('-S', '-b', '-o', bamfile, samfile, catch_stdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 7SK reads from BAM files\n",
    "BAM = []\n",
    "for file in glob.glob('./*_polyAfiltered.bam'):\n",
    "    BAM.append(file)\n",
    "\n",
    "for bamfile in BAM:\n",
    "    name, ext = os.path.splitext(bamfile)\n",
    "    name_7sk = \"{name}_{id}{ext}\".format(name=name, id='7SK_only', ext=ext)\n",
    "    name_no_7sk = \"{name}_{id}{ext}\".format(name=name, id='no_7SK', ext=ext)\n",
    "    pysam.view('-b', '-L', Rn7sk, '-U', name_no_7sk, bamfile, '-o', name_7sk, catch_stdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and index BAM files\n",
    "BAM = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK.bam'):\n",
    "    BAM.append(file)\n",
    "    \n",
    "for bamfile in BAM:\n",
    "    name, ext = os.path.splitext(bamfile)\n",
    "    bamfileSorted = \"{name}_{id}{ext}\".format(name=name, id='sorted', ext=ext)\n",
    "    pysam.sort('-o', bamfileSorted, bamfile, catch_stdout=False)\n",
    "    pysam.index(bamfileSorted, catch_stdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BAM files to BED12    \n",
    "sortedBAM = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted.bam'):\n",
    "    sortedBAM.append(file)\n",
    "\n",
    "for file in sortedBAM:\n",
    "    name, ext = os.path.splitext(file)\n",
    "    bedfile = \"{name}{ext}\".format(name=name, ext='.bed')\n",
    "    \n",
    "    bam_file = pybedtools.BedTool(file)\n",
    "    bedFile = bam_file.bam_to_bed(bed12 = True).saveas(bedfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-unique intermediates from BED12 files\n",
    "BED = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted.bed'):\n",
    "    BED.append(file)\n",
    "    \n",
    "for file in BED:\n",
    "    filter_nonunique_reads(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter splicing intermediates from BED12 files\n",
    "introns_5SS_bedtool = pybedtools.BedTool('introns_5SS.bed')\n",
    "\n",
    "BED = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted_unique.bed'):\n",
    "    BED.append(file)\n",
    "\n",
    "for file in BED:\n",
    "    filter_splicing_intermediates(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: combine replicates into untreated and DMSO-treated files\n",
    "\n",
    "untreated = []\n",
    "dmso = []\n",
    "for file in glob.glob('./*untreated*_polyAfiltered_no_7SK_sorted_unique_no_splicing_int.bed'):\n",
    "    untreated.append(file)\n",
    "for file in glob.glob('./*DMSO*_polyAfiltered_no_7SK_sorted_unique_no_splicing_int.bed'):\n",
    "    dmso.append(file)\n",
    "    \n",
    "with open('untreated_combined.bed', 'w') as outfile:\n",
    "    for fname in untreated:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "                \n",
    "with open('dmso_combined.bed', 'w') as outfile:\n",
    "    for fname in dmso:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reads in each file along the way\n",
    "\n",
    "input_count = []\n",
    "for file in samFiles:\n",
    "    samfile = pysam.AlignmentFile(file, \"rb\")\n",
    "    count = samfile.count()\n",
    "    input_count.append(count)\n",
    "    \n",
    "polyA_filtered_count = []\n",
    "for file in glob.glob('./*_polyAfiltered.sam'):\n",
    "    samfile = pysam.AlignmentFile(file, \"rb\")\n",
    "    count = samfile.count()\n",
    "    polyA_filtered_count.append(count)\n",
    "    \n",
    "    \n",
    "no_7sk_count = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted.bam'):\n",
    "    bamfile = pysam.AlignmentFile(file, \"rb\")\n",
    "    count = bamfile.count()\n",
    "    no_7sk_count.append(count)\n",
    "        \n",
    "unique_reads_count = []\n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted_unique.bed'):\n",
    "    count = len(open(file).readlines())\n",
    "    unique_reads_count.append(count)\n",
    "    \n",
    "no_splicing_int_count = []  \n",
    "for file in glob.glob('./*_polyAfiltered_no_7SK_sorted_unique_no_splicing_int.bed'):\n",
    "    count = len(open(file).readlines())\n",
    "    no_splicing_int_count.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table of read counts that are filtered at each step\n",
    "\n",
    "counts_df = pd.DataFrame(list(zip(samFiles, input_count, polyA_filtered_count, no_7sk_count, unique_reads_count, no_splicing_int_count)), \n",
    "                        columns =['Sample', 'Mapped', 'PolyA', '7SK', 'Non-unique Reads', 'Splicing Intermediates'])\n",
    "\n",
    "counts_df.to_csv('filtering_stats.csv', \n",
    "               sep = '\\t', \n",
    "               index = True, \n",
    "               header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mapped reads is: 1155629\n",
      "Percent of polyA reads filtered is: 1.67043229271678\n",
      "Percent of 7SK reads filtered is: 24.880216747762475\n",
      "Percent of non-unique reads filtered is: 14.23648939235689\n",
      "Percent of splicing intermediates reads filtered is: 4.155572419868314\n",
      "Percent of total reads filtered is: 40.787138432836144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kirsten/Applications/anaconda3/envs/nanoCOP/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Add a row with column totals\n",
    "\n",
    "# counts_df = pd.read_csv('filtering_stats.csv', delimiter = '\\t', index_col = 0)\n",
    "counts_df.loc['Total']= counts_df.sum()\n",
    "counts_df['Sample']['Total'] = 'Total'\n",
    "counts_df\n",
    "\n",
    "# Print a report on the number of reads filtered at each step\n",
    "\n",
    "mapped = counts_df['Mapped']['Total']\n",
    "polyA = counts_df['PolyA']['Total']\n",
    "sevenSK = counts_df['7SK']['Total']\n",
    "non_unique = counts_df['Non-unique Reads']['Total']\n",
    "spl_int = counts_df['Splicing Intermediates']['Total']\n",
    "\n",
    "print('Number of mapped reads is: ' + str(mapped))\n",
    "print('Percent of polyA reads filtered is: ' + str(((mapped-polyA)/mapped)*100))\n",
    "print('Percent of 7SK reads filtered is: ' + str(((polyA-sevenSK)/mapped)*100))\n",
    "print('Percent of non-unique reads filtered is: ' + str(((sevenSK-non_unique)/mapped)*100))\n",
    "print('Percent of splicing intermediates reads filtered is: ' + str(((non_unique-spl_int)/mapped)*100))\n",
    "print('Percent of total reads filtered is: ' + str(((mapped-non_unique)/mapped)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Mapped</th>\n",
       "      <th>PolyA</th>\n",
       "      <th>7SK</th>\n",
       "      <th>Non-unique Reads</th>\n",
       "      <th>Splicing Intermediates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_untreated_RSII.sam</td>\n",
       "      <td>21585</td>\n",
       "      <td>21389</td>\n",
       "      <td>16260</td>\n",
       "      <td>14994</td>\n",
       "      <td>14392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_untreated_SQ.sam</td>\n",
       "      <td>246281</td>\n",
       "      <td>244321</td>\n",
       "      <td>177301</td>\n",
       "      <td>163876</td>\n",
       "      <td>158192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_untreated_RSII.sam</td>\n",
       "      <td>42449</td>\n",
       "      <td>41894</td>\n",
       "      <td>33544</td>\n",
       "      <td>32497</td>\n",
       "      <td>29964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_untreated_SQ.sam</td>\n",
       "      <td>273317</td>\n",
       "      <td>269003</td>\n",
       "      <td>225904</td>\n",
       "      <td>217732</td>\n",
       "      <td>199768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_DMSO_RSII.sam</td>\n",
       "      <td>19878</td>\n",
       "      <td>19526</td>\n",
       "      <td>14867</td>\n",
       "      <td>9076</td>\n",
       "      <td>8566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3_DMSO_SQ.sam</td>\n",
       "      <td>268953</td>\n",
       "      <td>265119</td>\n",
       "      <td>177619</td>\n",
       "      <td>114676</td>\n",
       "      <td>108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4_DMSO_RSII.sam</td>\n",
       "      <td>27220</td>\n",
       "      <td>26700</td>\n",
       "      <td>19595</td>\n",
       "      <td>13036</td>\n",
       "      <td>11815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4_DMSO_SQ.sam</td>\n",
       "      <td>255946</td>\n",
       "      <td>248373</td>\n",
       "      <td>183712</td>\n",
       "      <td>118394</td>\n",
       "      <td>105161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>Total</td>\n",
       "      <td>1155629</td>\n",
       "      <td>1136325</td>\n",
       "      <td>848802</td>\n",
       "      <td>684281</td>\n",
       "      <td>636258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Sample   Mapped    PolyA     7SK  Non-unique Reads  \\\n",
       "0      1_untreated_RSII.sam    21585    21389   16260             14994   \n",
       "1        1_untreated_SQ.sam   246281   244321  177301            163876   \n",
       "2      2_untreated_RSII.sam    42449    41894   33544             32497   \n",
       "3        2_untreated_SQ.sam   273317   269003  225904            217732   \n",
       "4           3_DMSO_RSII.sam    19878    19526   14867              9076   \n",
       "5             3_DMSO_SQ.sam   268953   265119  177619            114676   \n",
       "6           4_DMSO_RSII.sam    27220    26700   19595             13036   \n",
       "7             4_DMSO_SQ.sam   255946   248373  183712            118394   \n",
       "Total                 Total  1155629  1136325  848802            684281   \n",
       "\n",
       "       Splicing Intermediates  \n",
       "0                       14392  \n",
       "1                      158192  \n",
       "2                       29964  \n",
       "3                      199768  \n",
       "4                        8566  \n",
       "5                      108400  \n",
       "6                       11815  \n",
       "7                      105161  \n",
       "Total                  636258  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt counts table from wide to long format for plotting\n",
    "df = pd.melt(counts_df, id_vars=['Sample'], value_vars=['Mapped', 'PolyA', '7SK', 'Splicing Intermediates', 'Non-unique Reads'])\n",
    "\n",
    "# add categorial variable to control the order of plotting\n",
    "variable_cat = pd.Categorical(df['variable'], categories = ['Mapped',\n",
    "                                                            'PolyA', \n",
    "                                                            '7SK',\n",
    "                                                            'Non-unique Reads', \n",
    "                                                            'Splicing Intermediates'])\n",
    "\n",
    "df = df.assign(variable_cat = variable_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kirsten/Applications/anaconda3/envs/nanoCOP/lib/python3.7/site-packages/plotnine/ggplot.py:729: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "  from_inches(height, units), units), PlotnineWarning)\n",
      "/Users/Kirsten/Applications/anaconda3/envs/nanoCOP/lib/python3.7/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: filtering_counts.pdf\n",
      "  warn('Filename: {}'.format(filename), PlotnineWarning)\n"
     ]
    }
   ],
   "source": [
    "# plot count values across all samples\n",
    "plt = (\n",
    "    ggplot(aes(x = 'variable_cat', y = 'value', fill = 'variable'), df) + \n",
    "    geom_bar(stat = 'identity', position = 'dodge') + \n",
    "    facet_wrap('Sample', scales = 'free_y') +\n",
    "    theme_classic() +\n",
    "    theme(subplots_adjust={'wspace':0.8}) +\n",
    "    theme(axis_text_x=element_text(rotation=45, hjust=1))\n",
    ")\n",
    "# plt\n",
    "plt.save(filename = 'filtering_counts.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
