{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook filters mapped HBB-targeted LRS data to remove:\n",
    "    polyadenylated transcripts,\n",
    "    non-unique reads,\n",
    "    and splicing intermeditates  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pysam\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from plotnine import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42 # export pdfs with editable font types in Illustrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data filenames and annotations used for filtering\n",
    "samFiles_GLOBE = [\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB1_GLOBE.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB2_GLOBE.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB3_GLOBE.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS1_GLOBE.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS2_GLOBE.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS3_GLOBE.sam'\n",
    "    ]\n",
    "\n",
    "samFiles_mm10 = [\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB1_mm10.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB2_mm10.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/HBB3_mm10.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS1_mm10.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS2_mm10.sam',\n",
    "    '../../0_mapped_data/HBB_tLRS/IVS3_mm10.sam'\n",
    "    ]\n",
    "\n",
    "# annotation of 5'SS coordinates for GLOBE and mm10 b-globin introns\n",
    "introns_5SS = '../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed'\n",
    "# save as bedtool object for intersect\n",
    "introns_5SS_bedtool = pybedtools.BedTool(introns_5SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter out polyadenylated and non-polyadenylated reads from tLRS data mapped to GLOBE HBB locus\n",
    "\n",
    "def filter_polyA_GLOBE(mapped_reads_file):\n",
    "    \n",
    "    def append_id_polyA(mapped_reads_file):\n",
    "        filename = os.path.basename(mapped_reads_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='polyA_softclipped', ext=ext)\n",
    "    \n",
    "    def append_id_polyAfiltered(mapped_reads_file):\n",
    "        filename = os.path.basename(mapped_reads_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='polyAfiltered_softclipped', ext=ext)\n",
    "   \n",
    "    output_pAfiltered = open(append_id_polyAfiltered(mapped_reads_file), 'w')\n",
    "    output_pA = open(append_id_polyA(mapped_reads_file), 'w')\n",
    "    \n",
    "    with open(mapped_reads_file, 'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            col = line.split('\\t')\n",
    "            if col[0][0] == '@': # write header lines into output file\n",
    "                output_pAfiltered.write(line + '\\n')\n",
    "                output_pA.write(line + '\\n')\n",
    "                continue\n",
    "            cigar = str(col[5]) # gets cigar string from SAM file\n",
    "            last100 = col[9][:100] # gets sequence of 100 leftmost bases in read\n",
    "            \n",
    "            if \"S\" in cigar: # if soft-clipping present in read and...\n",
    "                if re.findall('[A-Z]', cigar)[0] == 'S': # ...if read contains soft-clipping as the first cigar operator\n",
    "                    index = cigar.find('S') # finds position of left-most S in cigar string\n",
    "                    length_clipped = int(cigar.split(\"S\")[0]) # get digits before first S in CIGAR string\n",
    "                    clipped_bases = col[9][:length_clipped] # get sequence of clipped bases from start to first S\n",
    "                    read_end = int(col[3]) - length_clipped # read end is start of mapped sequence plus clipped bases (subtract because all reads are on - strand)\n",
    "                    clip_start = int(col[3]) - 1 # start of mapped sequence minus 1\n",
    "                    \n",
    "                    if read_end < 2448 and last100.count(\"T\") >= 70: # if soft clipping begins after the HBB insertion (mapping to downstream genomic region)\n",
    "                        continue # this filters out readthrough reads with polyT tails\n",
    "                    elif clip_start in range(4426,4480) and clipped_bases.count('T')/len(clipped_bases) >=0.7: # if clipping starts within 50nt of canonical polyA site and clipped bases are greater than 0.7T\n",
    "                        output_pA.write(line + \"\\n\") # write out normal polyT reads\n",
    "                    elif clip_start in range(4426,4480) and clipped_bases.count('T') >=4: # if clipping starts within 50 nt of canonical polyA site and has more than 4T (accounts for even very small tails)\n",
    "                        output_pA.write(line + \"\\n\") # write out normal polyT reads with very small tails\n",
    "                    elif read_end > 2448 and length_clipped >= 20: # any other long mismatch within GLOBE sequnce\n",
    "                        continue # this gets rid of chimeric reads and weird mapping\n",
    "                    elif read_end < 2448 and clip_start > 2448: # any other long mismatch within GLOBE sequence\n",
    "                        continue # other long chimeric reads\n",
    "                    else:\n",
    "                        output_pAfiltered.write(line + \"\\n\")\n",
    "                else:\n",
    "                    output_pAfiltered.write(line + \"\\n\")\n",
    "            else:\n",
    "                output_pAfiltered.write(line + \"\\n\")\n",
    "                        \n",
    "    f.close()\n",
    "    output_pAfiltered.close()\n",
    "    output_pA.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter out polyadenylated and non-polyadenylated reads from tLRS data mapped to mm10 Hbb locus\n",
    "\n",
    "def filter_polyA_mm10(mapped_reads_file):\n",
    "    \n",
    "    def append_id_polyA(mapped_reads_file):\n",
    "        filename = os.path.basename(mapped_reads_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='polyA', ext=ext)\n",
    "    \n",
    "    def append_id_polyAfiltered(mapped_reads_file):\n",
    "        filename = os.path.basename(mapped_reads_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='polyAfiltered', ext=ext)\n",
    "   \n",
    "    output_pAfiltered = open(append_id_polyAfiltered(mapped_reads_file), 'w')\n",
    "    output_pA = open(append_id_polyA(mapped_reads_file), 'w')\n",
    "    \n",
    "    with open(mapped_reads_file, 'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            col = line.split('\\t')\n",
    "            if col[0][0] == '@': # write header lines into output file\n",
    "                output_pAfiltered.write(line + '\\n')\n",
    "                output_pA.write(line + '\\n')\n",
    "                continue\n",
    "            cigar = str(col[5]) # gets cigar string from SAM file\n",
    "            last100 = col[9][:100] # gets sequence of 100 leftmost bases in read\n",
    "            \n",
    "            if \"S\" in cigar: # if soft-clipping present in read and...\n",
    "                if re.findall('[A-Z]', cigar)[0] == 'S': # ...if read contains soft-clipping as the first cigar operator\n",
    "                    index = cigar.find('S') # finds position of left-most S in cigar string\n",
    "                    length_clipped = int(cigar.split(\"S\")[0]) # get digits before first S in CIGAR string\n",
    "                    clipped_bases = col[9][:length_clipped] # get sequence of clipped bases from start to first S\n",
    "                    read_end = int(col[3]) - length_clipped # read end is start of mapped sequence plus clipped bases (subtract because all reads are on - strand)\n",
    "                    clip_start = int(col[3]) - 1 # start of mapped sequence minus 1\n",
    "                    \n",
    "                    if clip_start in range(103826506,103826556) and clipped_bases.count('T')/len(clipped_bases) >=0.7: # if clipping starts within 50nt of canonical polyA site and clipped bases are greater than 70% T\n",
    "                        output_pA.write(line + \"\\n\") # write out normal polyT reads\n",
    "                    elif clip_start in range(103826506,103826556) and clipped_bases.count('T') >=4: # if clipping starts within 50 nt of canonical polyA site and has more than 4T (accounts for even very small tails)\n",
    "                        output_pA.write(line + \"\\n\") # write out normal polyT reads with very small tails\n",
    "                    elif read_end < 103826523 and last100.count(\"T\") >= 70: # if soft clipping begins after the end of the gene\n",
    "                        output_pA.write(line + \"\\n\") #filter reads that very rarely use a downstream polyA site\n",
    "                    elif read_end > 103826523 and length_clipped >= 20: # any other long mismatch within Hbb sequnce\n",
    "                        continue\n",
    "                    else:\n",
    "                        output_pAfiltered.write(line + \"\\n\")\n",
    "                else:\n",
    "                    output_pAfiltered.write(line + \"\\n\")\n",
    "            else:\n",
    "                output_pAfiltered.write(line + \"\\n\")\n",
    "                        \n",
    "    f.close()\n",
    "    output_pAfiltered.close()\n",
    "    output_pA.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fill in soft-clipped bases past the end of the GLOBE annotation for reads mapped to GLOBE\n",
    "def fill_soft_clipping(sam_file):\n",
    "    \n",
    "    def append_id_filled(sam_file):\n",
    "        name, ext = os.path.splitext(sam_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='filled', ext=ext)\n",
    "\n",
    "    output_filled = open(append_id_filled(sam_file), 'w')\n",
    "    \n",
    "    with open(sam_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            col = line.split('\\t')\n",
    "            if col[0][0] == '@': # write header lines into output file\n",
    "                output_filled.write(line + \"\\n\")\n",
    "                continue\n",
    "            cigar = str(col[5]) # gets cigar string from SAM file\n",
    "            map_start = col[3] # position of mapping start (end of soft-clipping)\n",
    "            if \"S\" in cigar: # if soft-clipping present in read and...\n",
    "                if re.findall('[A-Z]', cigar)[0] == 'S' and re.findall('[A-Z]', cigar)[1] == 'M': # ...if first cigar operator is 'S' and next operator is 'M'\n",
    "                    position_first_S = cigar.find('S') # finds position of left-most 'S' in cigar string\n",
    "                    length_first_S = int(cigar.split(\"S\")[0]) # get digits before first 'S' in CIGAR string\n",
    "                    position_first_M = cigar.find('M') # find first M\n",
    "                    length_first_M = int(cigar.split(\"M\")[0][position_first_S+1:]) # get length of first M\n",
    "                    new_M_length = length_first_S + length_first_M\n",
    "                    remainder = cigar[position_first_M+1:]\n",
    "                    new_cigar = str(new_M_length) + \"M\" + str(remainder)\n",
    "                    new_POS = int(map_start) - int(length_first_S)\n",
    "                    \n",
    "                    output_filled.write(col[0] + \"\\t\" + col[1] + \"\\t\" + col[2] + \"\\t\" + str(new_POS) + \"\\t\" + col[4] + \"\\t\" + str(new_cigar) + \"\\t\" + col[6] + \"\\t\" + col[7] + \"\\t\" + col[8] + \"\\t\" + col[9] + \"\\t\" + col[10] + \"\\n\")\n",
    "                else:\n",
    "                    output_filled.write(line + \"\\n\")\n",
    "            else:\n",
    "                output_filled.write(line + \"\\n\")\n",
    "    \n",
    "    output_filled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for filtering non-unique readnames from each data file\n",
    "def filter_nonunique_reads(bed_file):\n",
    "    \n",
    "    def name_unique_reads(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='unique', ext=ext)\n",
    "    \n",
    "    # first open and reorder coordinates of bed file to put 3'end in position for intersection\n",
    "    all_data = pd.read_csv(bed_file, delimiter = '\\t', names =  ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'])\n",
    "    grouped = all_data.groupby(['name']).size().to_frame(name = 'count').reset_index()\n",
    "\n",
    "    # get read names that are unique and filter to keep only reads which have name count == 1\n",
    "    is_unique =  grouped['count'] == 1\n",
    "    unique = grouped[is_unique]\n",
    "    unique_names = pd.Series(unique['name'].values) # create a series of readnames that have occur only once\n",
    "\n",
    "    data_is_unique = all_data['name'].isin(unique_names)\n",
    "    data_unique = all_data[data_is_unique] # filter data for readnames that are unique\n",
    "    \n",
    "    # save unique reads to a new file\n",
    "    data_unique.to_csv(name_unique_reads(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for filtering splicing intermediates from each data file\n",
    "def filter_splicing_intermediates(bed_file):\n",
    "    \n",
    "    def name_file_no_splicing_int(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='no_splicing_int', ext=ext)\n",
    "    \n",
    "    def name_file_splicing_int(bed_file):\n",
    "        name, ext = os.path.splitext(bed_file)\n",
    "        return \"{name}_{id}{ext}\".format(name=name, id='splicing_int', ext=ext)\n",
    "    \n",
    "    # first open and reorder coordinates of bed file to put 3'end in position for intersection\n",
    "    data = pd.read_csv(bed_file, delimiter = '\\t', names =  ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'])\n",
    "    data.loc[data['strand'] == '+', 'threeEnd'] = data['end']\n",
    "    data.loc[data['strand'] == '-', 'threeEnd'] = data['start']\n",
    "    data.loc[data['strand'] == '+', 'fiveEnd'] = data['start']\n",
    "    data.loc[data['strand'] == '-', 'fiveEnd'] = data['end']\n",
    "    data.loc[data['strand'] == '+', 'newStart'] = data['threeEnd'] - 1\n",
    "    data.loc[data['strand'] == '-', 'newStart'] = data['threeEnd']\n",
    "    data.loc[data['strand'] == '+', 'newEnd'] = data['threeEnd']\n",
    "    data.loc[data['strand'] == '-', 'newEnd'] = data['threeEnd'] + 1\n",
    "\n",
    "    # convert coordinates back to integer values\n",
    "    data['newStart'] = data['newStart'].astype(np.int64)\n",
    "    data['newEnd'] = data['newEnd'].astype(np.int64)\n",
    "    data['fiveEnd'] = data['fiveEnd'].astype(np.int64)\n",
    "    data['threeEnd'] = data['threeEnd'].astype(np.int64)\n",
    "    \n",
    "    # save a temporary bed file with data 3'end coordinates\n",
    "    data.to_csv('tmp.bed', \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'], \n",
    "               header = False)\n",
    "        \n",
    "    # intersect data 3'end with intron 5'SS coordinates to get splicing intermediates and non-intermediates\n",
    "    tmp_bedfile = open('tmp.bed')\n",
    "    data_bedtool = pybedtools.BedTool(tmp_bedfile)\n",
    "    intersect1 = data_bedtool.intersect(introns_5SS_bedtool, u = True).saveas('tmp_splicing_int.bed')\n",
    "    \n",
    "    tmp_bedfile = open('tmp.bed')\n",
    "    data_bedtool = pybedtools.BedTool(tmp_bedfile)\n",
    "    intersect2 = data_bedtool.intersect(introns_5SS_bedtool, v = True).saveas('tmp_no_splicing_int.bed')\n",
    "\n",
    "    # reorder coordinates of data files\n",
    "    data1 = pd.read_csv('tmp_splicing_int.bed', delimiter = '\\t', names =  ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'])\n",
    "    data1.to_csv(name_file_splicing_int(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)\n",
    "    \n",
    "    data2 = pd.read_csv('tmp_no_splicing_int.bed', delimiter = '\\t', names =  ['chr', 'newStart', 'newEnd', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts', 'start', 'end'])\n",
    "    data2.to_csv(name_file_no_splicing_int(bed_file), \n",
    "               sep = '\\t', \n",
    "               index = False, \n",
    "               columns = ['chr', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb', 'blockCount', 'blockSizes', 'blockStarts'], \n",
    "               header = False)\n",
    "    \n",
    "    # clean up temp files\n",
    "    os.remove('tmp.bed')\n",
    "    os.remove('tmp_no_splicing_int.bed')\n",
    "    os.remove('tmp_splicing_int.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter polyadenylated reads\n",
    "for file in samFiles_GLOBE:\n",
    "    filter_polyA_GLOBE(file)\n",
    "    \n",
    "for file in samFiles_mm10:\n",
    "    filter_polyA_mm10(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in soft-clipped bases for polyAfiltered GLOBE reads\n",
    "soft_clipped = []\n",
    "for file in glob.glob('./*_softclipped.sam'):\n",
    "    soft_clipped.append(file)\n",
    "    \n",
    "for file in soft_clipped:\n",
    "    fill_soft_clipping(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SAM files to BAM  files for further filtering\n",
    "SAM = []\n",
    "for file in glob.glob('./*_GLOBE_polyA_softclipped_filled.sam'):\n",
    "    SAM.append(file)\n",
    "for file in glob.glob('./*_GLOBE_polyAfiltered_softclipped_filled.sam'):\n",
    "    SAM.append(file)\n",
    "for file in glob.glob('./*_mm10_polyA.sam'):\n",
    "    SAM.append(file)\n",
    "for file in glob.glob('./*_mm10_polyAfiltered.sam'):\n",
    "    SAM.append(file)\n",
    "    \n",
    "for samfile in SAM:\n",
    "    name, ext = os.path.splitext(samfile)\n",
    "    bamfile = \"{name}{ext}\".format(name=name, ext='.bam')\n",
    "    pysam.view('-S', '-b', '-o', bamfile, samfile, catch_stdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and index BAM files\n",
    "BAM = []\n",
    "for file in glob.glob('./*.bam'):\n",
    "    BAM.append(file)\n",
    "    \n",
    "for bamfile in BAM:\n",
    "    name, ext = os.path.splitext(bamfile)\n",
    "    bamfileSorted = \"{name}_{id}{ext}\".format(name=name, id='sorted', ext=ext)\n",
    "    pysam.sort('-o', bamfileSorted, bamfile, catch_stdout=False)\n",
    "    pysam.index(bamfileSorted, catch_stdout=False)\n",
    "    \n",
    "# Convert BAM files to BED12    \n",
    "sortedBAM = []\n",
    "for file in glob.glob('./*_sorted.bam'):\n",
    "    sortedBAM.append(file)\n",
    "\n",
    "for file in sortedBAM:\n",
    "    name, ext = os.path.splitext(file)\n",
    "    bedfile = \"{name}{ext}\".format(name=name, ext='.bed')\n",
    "    \n",
    "    bam_file = pybedtools.BedTool(file)\n",
    "    bedFile = bam_file.bam_to_bed(bed12 = True).saveas(bedfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-unique intermediates from BED12 files\n",
    "BED_polyA = []\n",
    "for file in glob.glob('./*_polyA_sorted.bed'):\n",
    "    BED_polyA.append(file)\n",
    "for file in glob.glob('./*_polyA_softclipped_filled_sorted.bed'):\n",
    "    BED_polyA.append(file)\n",
    "    \n",
    "BED_polyAfiltered = []\n",
    "for file in glob.glob('./*_polyAfiltered_sorted.bed'):\n",
    "    BED_polyAfiltered.append(file)\n",
    "for file in glob.glob('./*_polyAfiltered_softclipped_filled_sorted.bed'):\n",
    "    BED_polyAfiltered.append(file)\n",
    "    \n",
    "for file in BED_polyA:\n",
    "    filter_nonunique_reads(file)\n",
    "for file in BED_polyAfiltered:\n",
    "    filter_nonunique_reads(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n",
      "***** WARNING: File ../../annotation_files/GLOBE_mm10_HBB_introns_5SS.bed has inconsistent naming convention for record:\n",
      "chr7\t103827783\t103827784\tintron_1_intermediate\t-\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter splicing intermediates from BED12 files\n",
    "BED_unique = []\n",
    "for file in glob.glob('./*_polyAfiltered_sorted_unique.bed'):\n",
    "    BED_unique.append(file)\n",
    "for file in glob.glob('./*_polyAfiltered_softclipped_filled_sorted_unique.bed'):\n",
    "    BED_unique.append(file)\n",
    "\n",
    "for file in BED_unique:\n",
    "    filter_splicing_intermediates(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] could not open alignment file `./HBB1_GLOBE_polyAfiltered.sam`: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fe2372e6fb80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpolyA_filtered_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./*_polyAfiltered.sam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msamfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlignmentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpolyA_filtered_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpysam/libcalignmentfile.pyx\u001b[0m in \u001b[0;36mpysam.libcalignmentfile.AlignmentFile.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpysam/libcalignmentfile.pyx\u001b[0m in \u001b[0;36mpysam.libcalignmentfile.AlignmentFile._open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] could not open alignment file `./HBB1_GLOBE_polyAfiltered.sam`: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Fix naming to include soft_clipped_filled data\n",
    "\n",
    "# # Count the number of reads in each file along the way\n",
    "\n",
    "# samFiles = []\n",
    "# for file in glob.glob('./*_GLOBE.sam'):\n",
    "#     samFiles.append(file)\n",
    "# for file in glob.glob('./*_mm10.sam'):\n",
    "#     samFiles.append(file)\n",
    "\n",
    "# input_count = []\n",
    "# for file in samFiles:\n",
    "#     samfile = pysam.AlignmentFile(file, \"rb\")\n",
    "#     count = samfile.count()\n",
    "#     input_count.append(count)\n",
    "    \n",
    "# polyA_filtered_count = []\n",
    "# for file in glob.glob('./*_polyAfiltered.sam'):\n",
    "#     samfile = pysam.AlignmentFile(file, \"rb\")\n",
    "#     count = samfile.count()\n",
    "#     polyA_filtered_count.append(count)\n",
    "    \n",
    "# polyA_count = []\n",
    "# for file in glob.glob('./*_polyA.sam'):\n",
    "#     samfile = pysam.AlignmentFile(file, \"rb\")\n",
    "#     count = samfile.count()\n",
    "#     polyA_count.append(count)\n",
    "        \n",
    "# polyA_unique_reads_count = []\n",
    "# for file in glob.glob('./*_polyA_sorted_unique.bed'):\n",
    "#     count = len(open(file).readlines())\n",
    "#     polyA_unique_reads_count.append(count)\n",
    "    \n",
    "# polyAfiltered_unique_reads_count = []\n",
    "# for file in glob.glob('./*_polyAfiltered_sorted_unique.bed'):\n",
    "#     count = len(open(file).readlines())\n",
    "#     polyAfiltered_unique_reads_count.append(count)\n",
    "    \n",
    "# no_splicing_int_count = []  \n",
    "# for file in glob.glob('./*_no_splicing_int.bed'):\n",
    "#     count = len(open(file).readlines())\n",
    "#     no_splicing_int_count.append(count)\n",
    "    \n",
    "# splicing_int_count = []\n",
    "# for file in glob.glob('./*_unique_splicing_int.bed'):\n",
    "#     count = len(open(file).readlines())\n",
    "#     splicing_int_count.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Mapped</th>\n",
       "      <th>PolyA (+)</th>\n",
       "      <th>PolyA (-)</th>\n",
       "      <th>PolyA (+) Unique Reads</th>\n",
       "      <th>PolyA (-) Unique Reads</th>\n",
       "      <th>Splicing Intermediates</th>\n",
       "      <th>Non-Intermediates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./HBB1_GLOBE.sam</td>\n",
       "      <td>24983</td>\n",
       "      <td>14450</td>\n",
       "      <td>9850</td>\n",
       "      <td>14450</td>\n",
       "      <td>9746</td>\n",
       "      <td>1122</td>\n",
       "      <td>8624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./IVS1_GLOBE.sam</td>\n",
       "      <td>25817</td>\n",
       "      <td>13258</td>\n",
       "      <td>9340</td>\n",
       "      <td>13258</td>\n",
       "      <td>9249</td>\n",
       "      <td>1071</td>\n",
       "      <td>8178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./IVS2_GLOBE.sam</td>\n",
       "      <td>26744</td>\n",
       "      <td>12741</td>\n",
       "      <td>9325</td>\n",
       "      <td>12741</td>\n",
       "      <td>9256</td>\n",
       "      <td>1088</td>\n",
       "      <td>8168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./HBB2_GLOBE.sam</td>\n",
       "      <td>23218</td>\n",
       "      <td>10871</td>\n",
       "      <td>14318</td>\n",
       "      <td>10871</td>\n",
       "      <td>14175</td>\n",
       "      <td>1326</td>\n",
       "      <td>12849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./HBB3_GLOBE.sam</td>\n",
       "      <td>22670</td>\n",
       "      <td>12710</td>\n",
       "      <td>13474</td>\n",
       "      <td>12710</td>\n",
       "      <td>13354</td>\n",
       "      <td>1256</td>\n",
       "      <td>12098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./IVS3_GLOBE.sam</td>\n",
       "      <td>26387</td>\n",
       "      <td>12275</td>\n",
       "      <td>13507</td>\n",
       "      <td>12275</td>\n",
       "      <td>13380</td>\n",
       "      <td>1403</td>\n",
       "      <td>11977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./HBB1_mm10.sam</td>\n",
       "      <td>24332</td>\n",
       "      <td>17215</td>\n",
       "      <td>7038</td>\n",
       "      <td>17215</td>\n",
       "      <td>6559</td>\n",
       "      <td>1687</td>\n",
       "      <td>4872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./HBB3_mm10.sam</td>\n",
       "      <td>21841</td>\n",
       "      <td>15104</td>\n",
       "      <td>6792</td>\n",
       "      <td>15104</td>\n",
       "      <td>6335</td>\n",
       "      <td>1573</td>\n",
       "      <td>4762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./IVS1_mm10.sam</td>\n",
       "      <td>26437</td>\n",
       "      <td>15307</td>\n",
       "      <td>6481</td>\n",
       "      <td>15307</td>\n",
       "      <td>6144</td>\n",
       "      <td>1680</td>\n",
       "      <td>4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./IVS2_mm10.sam</td>\n",
       "      <td>26227</td>\n",
       "      <td>16009</td>\n",
       "      <td>10343</td>\n",
       "      <td>16009</td>\n",
       "      <td>9948</td>\n",
       "      <td>1215</td>\n",
       "      <td>8733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./HBB2_mm10.sam</td>\n",
       "      <td>21948</td>\n",
       "      <td>15461</td>\n",
       "      <td>10693</td>\n",
       "      <td>15461</td>\n",
       "      <td>10256</td>\n",
       "      <td>1519</td>\n",
       "      <td>8737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./IVS3_mm10.sam</td>\n",
       "      <td>26808</td>\n",
       "      <td>15672</td>\n",
       "      <td>11065</td>\n",
       "      <td>15672</td>\n",
       "      <td>10614</td>\n",
       "      <td>1607</td>\n",
       "      <td>9007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>Total</td>\n",
       "      <td>297412</td>\n",
       "      <td>171073</td>\n",
       "      <td>122226</td>\n",
       "      <td>171073</td>\n",
       "      <td>119016</td>\n",
       "      <td>16547</td>\n",
       "      <td>102469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Sample  Mapped  PolyA (+)  PolyA (-)  PolyA (+) Unique Reads  \\\n",
       "0      ./HBB1_GLOBE.sam   24983      14450       9850                   14450   \n",
       "1      ./IVS1_GLOBE.sam   25817      13258       9340                   13258   \n",
       "2      ./IVS2_GLOBE.sam   26744      12741       9325                   12741   \n",
       "3      ./HBB2_GLOBE.sam   23218      10871      14318                   10871   \n",
       "4      ./HBB3_GLOBE.sam   22670      12710      13474                   12710   \n",
       "5      ./IVS3_GLOBE.sam   26387      12275      13507                   12275   \n",
       "6       ./HBB1_mm10.sam   24332      17215       7038                   17215   \n",
       "7       ./HBB3_mm10.sam   21841      15104       6792                   15104   \n",
       "8       ./IVS1_mm10.sam   26437      15307       6481                   15307   \n",
       "9       ./IVS2_mm10.sam   26227      16009      10343                   16009   \n",
       "10      ./HBB2_mm10.sam   21948      15461      10693                   15461   \n",
       "11      ./IVS3_mm10.sam   26808      15672      11065                   15672   \n",
       "Total             Total  297412     171073     122226                  171073   \n",
       "\n",
       "       PolyA (-) Unique Reads  Splicing Intermediates  Non-Intermediates  \n",
       "0                        9746                    1122               8624  \n",
       "1                        9249                    1071               8178  \n",
       "2                        9256                    1088               8168  \n",
       "3                       14175                    1326              12849  \n",
       "4                       13354                    1256              12098  \n",
       "5                       13380                    1403              11977  \n",
       "6                        6559                    1687               4872  \n",
       "7                        6335                    1573               4762  \n",
       "8                        6144                    1680               4464  \n",
       "9                        9948                    1215               8733  \n",
       "10                      10256                    1519               8737  \n",
       "11                      10614                    1607               9007  \n",
       "Total                  119016                   16547             102469  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a table of read counts that are filtered at each step\n",
    "\n",
    "counts_df = pd.DataFrame(list(zip(samFiles, input_count, polyA_count, polyA_filtered_count, polyA_unique_reads_count, polyAfiltered_unique_reads_count, splicing_int_count, no_splicing_int_count)), \n",
    "                        columns =['Sample', 'Mapped', 'PolyA (+)', 'PolyA (-)', 'PolyA (+) Unique Reads', 'PolyA (-) Unique Reads', 'Splicing Intermediates', 'Non-Intermediates'])\n",
    "\n",
    "# Add a row with column totals\n",
    "counts_df.loc['Total']= counts_df.sum()\n",
    "counts_df['Sample']['Total'] = 'Total'\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>variable_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./HBB1_GLOBE.sam</td>\n",
       "      <td>Mapped</td>\n",
       "      <td>24983</td>\n",
       "      <td>Mapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./IVS1_GLOBE.sam</td>\n",
       "      <td>Mapped</td>\n",
       "      <td>25817</td>\n",
       "      <td>Mapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./IVS2_GLOBE.sam</td>\n",
       "      <td>Mapped</td>\n",
       "      <td>26744</td>\n",
       "      <td>Mapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./HBB2_GLOBE.sam</td>\n",
       "      <td>Mapped</td>\n",
       "      <td>23218</td>\n",
       "      <td>Mapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./HBB3_GLOBE.sam</td>\n",
       "      <td>Mapped</td>\n",
       "      <td>22670</td>\n",
       "      <td>Mapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>./IVS1_mm10.sam</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "      <td>4464</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>./IVS2_mm10.sam</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "      <td>8733</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>./HBB2_mm10.sam</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "      <td>8737</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>./IVS3_mm10.sam</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "      <td>9007</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Total</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "      <td>102469</td>\n",
       "      <td>Non-Intermediates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sample           variable   value       variable_cat\n",
       "0   ./HBB1_GLOBE.sam             Mapped   24983             Mapped\n",
       "1   ./IVS1_GLOBE.sam             Mapped   25817             Mapped\n",
       "2   ./IVS2_GLOBE.sam             Mapped   26744             Mapped\n",
       "3   ./HBB2_GLOBE.sam             Mapped   23218             Mapped\n",
       "4   ./HBB3_GLOBE.sam             Mapped   22670             Mapped\n",
       "..               ...                ...     ...                ...\n",
       "86   ./IVS1_mm10.sam  Non-Intermediates    4464  Non-Intermediates\n",
       "87   ./IVS2_mm10.sam  Non-Intermediates    8733  Non-Intermediates\n",
       "88   ./HBB2_mm10.sam  Non-Intermediates    8737  Non-Intermediates\n",
       "89   ./IVS3_mm10.sam  Non-Intermediates    9007  Non-Intermediates\n",
       "90             Total  Non-Intermediates  102469  Non-Intermediates\n",
       "\n",
       "[91 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melt counts table from wide to long format for plotting\n",
    "df = pd.melt(counts_df, id_vars=['Sample'], value_vars=['Mapped', 'PolyA (+)', 'PolyA (-)', 'PolyA (+) Unique Reads', 'PolyA (-) Unique Reads', 'Splicing Intermediates', 'Non-Intermediates'])\n",
    "\n",
    "# add categorial variable to control the order of plotting\n",
    "variable_cat = pd.Categorical(df['variable'], categories = ['Mapped', 'PolyA (+)', 'PolyA (-)', 'PolyA (+) Unique Reads', 'PolyA (-) Unique Reads', 'Splicing Intermediates', 'Non-Intermediates'])\n",
    "\n",
    "df = df.assign(variable_cat = variable_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot count values across all samples\n",
    "plt = (\n",
    "    ggplot(aes(x = 'variable_cat', y = 'value', fill = 'variable'), df) + \n",
    "    geom_bar(stat = 'identity', position = 'dodge') + \n",
    "    facet_wrap('Sample', scales = 'free_y') +\n",
    "    theme_classic() +\n",
    "    theme(subplots_adjust={'wspace':0.8}) +\n",
    "    theme(axis_text_x=element_text(rotation=45, hjust=1))\n",
    ")\n",
    "plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.save(filename = 'tLSR_filtering_counts.pdf')\n",
    "counts_df.to_csv('tLSR_filtering_stats.csv', \n",
    "               sep = '\\t', \n",
    "               index = True, \n",
    "               header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
